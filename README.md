
# Research Question
How effective is the developed visual model in recognizing protesters, describing their activities, estimating the level of perceived violence, and identifying exhibited emotions in protest images collected from social media platforms? How does the multi-task convolutional neural network perform in classifying the presence of protesters, predicting visual attributes, perceived violence, and exhibited emotions?

## Group 
- Karla Montano
- Rebeca Leon

# Introduction
In recent years, social media has become a powerful platform for documenting and disseminating images and information about protest movements worldwide. Our project, "Analyzing Social Media Protest Imagery: Multi-Task CNN for Detection, Violence Estimation, and Emotion Recognition," seeks to leverage this wealth of visual data to understand the dynamics of protests better. We aim to develop a robust visual model capable of recognizing protesters, describing their activities, estimating perceived violence, and identifying exhibited emotions in protest images collected from social media platforms. By employing a multi-task convolutional neural network (CNN), we will assess the model's performance in classifying the presence of protesters, predicting visual attributes, and discerning emotional and violent content. Utilizing a dataset comprising geotagged tweets and images from 2013 to 2017, annotated with various attributes, we will process and analyze this data to uncover insights into protest activities. Our methodology includes advanced image processing, data augmentation, and sentiment analysis techniques, with a structured approach to model training, validation, and optimization. Through this project, we aim to provide a comprehensive analysis of protest imagery, contributing valuable insights into the nature and impact of social movements as captured through the lens of social media.

# Methods 
- Convolutional Neural Network (CNN) for Image Clarification 
- Natural Language Processing (NLP) for Sentiment Analysis

## Analysis and Summary
The primary objective of this project is to develop and evaluate a visual model that can recognize protesters, describe their activities, estimate the level of perceived violence, and identify exhibited emotions in protest images collected from social media platforms. This analysis focuses on understanding the effectiveness of a multi-task convolutional neural network (CNN) in classifying the presence of protesters, predicting visual attributes, perceived violence, and exhibited emotions. We started by mounting Google Drive to access the dataset stored in specific directories for training and testing. Then we ensured the accessibility of the directories containing the images and listed their contents to verify successful mounting. Then we loaded and preprocessed the images by resizing them to a uniform size, normalizing pixel values, and organizing them into arrays. Then we split the training data into training and validation sets to evaluate the model's performance during training. We used TensorFlow’s tf.data.Dataset API to create data pipelines for training, validation, and testing. We developed a CNN model using the VGG16 architecture pre-trained on ImageNet as the base model. Added fully connected layers for the specific classification tasks. We trained the CNN model on the preprocessed training data and validated it on the validation data. We evaluated the model's performance on the test dataset and visualized the training results. Then we plotted the training and validation accuracy over the epochs to observe the model’s learning progress. The notebook successfully demonstrates the development of a multi-task CNN model for detecting protesters, estimating perceived violence, and recognizing exhibited emotions in protest images. The process included, Data preprocessing and augmentation to ensure robust training, Model selection and training using transfer learning with the VGG16 architecture, Evaluation and optimization of the model to achieve satisfactory performance metrics, and Visualization of the model's performance to interpret its learning behavior. Overall, the model achieved a reasonable accuracy in classifying protest-related images, indicating the potential of multi-task CNNs in analyzing complex visual data from social media platforms. Further optimization and fine-tuning could enhance the model's performance and extend its applicability to broader datasets and additional tasks.

## References 
- https://dl.acm.org/doi/pdf/10.1145/3123266.3123282
- https://github.com/wondonghyeon/protest-detection-violence-estimation

Donghyeon Won, Zachary C. Steinert-Threlkeld, and Jungseock Joo. 2017. Protest Activity Detection and Perceived Violence Estimation from Social Media Images. In Proceedings of the 25th ACM International Conference on Multimedia. ACM, 786-794.

