
# Research Question
How effective is the developed visual model in recognizing protesters, describing their activities, estimating the level of perceived violence, and identifying exhibited emotions in protest images collected from social media platforms? How does the multi-task convolutional neural network perform in classifying the presence of protesters, predicting visual attributes, perceived violence, and exhibited emotions?

### Group 
- Karla Montano
- Rebeca Leon

# Introduction
In recent years, social media has become a powerful platform for documenting and disseminating images and information about protest movements worldwide. Our project, "Analyzing Social Media Protest Imagery: Multi-Task CNN for Detection, Violence Estimation, and Emotion Recognition," seeks to leverage this wealth of visual data to understand the dynamics of protests better. We aim to develop a robust visual model capable of recognizing protesters, describing their activities, estimating perceived violence, and identifying exhibited emotions in protest images collected from social media platforms. By employing a multi-task convolutional neural network (CNN), we will assess the model's performance in classifying the presence of protesters, predicting visual attributes, and discerning emotional and violent content. Utilizing a dataset comprising geotagged tweets and images from 2013 to 2017, annotated with various attributes, we will process and analyze this data to uncover insights into protest activities. Our methodology includes advanced image processing, data augmentation, and sentiment analysis techniques, with a structured approach to model training, validation, and optimization. Through this project, we aim to provide a comprehensive analysis of protest imagery, contributing valuable insights into the nature and impact of social movements as captured through the lens of social media.

### Methods 
- Convolutional Neural Network (CNN) for Image Clarification 
- Natural Language Processing (NLP) for Sentiment Analysis

## Analysis
The primary objective of this project is to develop and evaluate a visual model that can recognize protesters, describe their activities, estimate the level of perceived violence, and identify exhibited emotions in protest images collected from social media platforms. This analysis focuses on understanding the effectiveness of a multi-task convolutional neural network (CNN) in classifying the presence of protesters, predicting visual attributes, perceived violence, and exhibited emotions. We started by mounting Google Drive to access the dataset stored in specific directories for training and testing. Then we ensured the accessibility of the directories containing the images and listed their contents to verify successful mounting. Then we loaded and preprocessed the images by resizing them to a uniform size, normalizing pixel values, and organizing them into arrays. Then we split the training data into training and validation sets to evaluate the model's performance during training. We used TensorFlow’s tf.data.Dataset API to create data pipelines for training, validation, and testing. We developed a CNN model using the VGG16 architecture pre-trained on ImageNet as the base model. Added fully connected layers for the specific classification tasks. We trained the CNN model on the preprocessed training data and validated it on the validation data. We evaluated the model's performance on the test dataset and visualized the training results. Then we plotted the training and validation accuracy over the epochs to observe the model’s learning progress. The notebook successfully demonstrates the development of a multi-task CNN model for detecting protesters, estimating perceived violence, and recognizing exhibited emotions in protest images. The process included, Data preprocessing and augmentation to ensure robust training, Model selection and training using transfer learning with the VGG16 architecture, Evaluation and optimization of the model to achieve satisfactory performance metrics, and Visualization of the model's performance to interpret its learning behavior. Overall, the model achieved a reasonable accuracy in classifying protest-related images, indicating the potential of multi-task CNNs in analyzing complex visual data from social media platforms. Further optimization and fine-tuning could enhance the model's performance and extend its applicability to broader datasets and additional tasks.

# Results 
Successfully mounted Google Drive and verified access to the directories containing the training and testing images. Implemented functions to load and preprocess images by resizing them to 150x150 pixels and normalizing the pixel values. The preprocessed images were organized into NumPy arrays for further processing. The dataset was split into training, validation, and testing sets to evaluate the model's performance at different stages. Created TensorFlow datasets for efficient data loading and batching during training and evaluation. Utilized the pre-trained VGG16 architecture as the base model, adding fully connected layers for the specific classification tasks. The base model's weights were frozen to leverage its learned features. Compiled the model using the Adam optimizer and sparse categorical cross-entropy loss function, with accuracy as the evaluation metric. Trained the CNN model on the preprocessed training data for 10 epochs, with validation on the validation set to monitor and fine-tune the model's performance. Evaluated the trained model on the test dataset, achieving a test accuracy of approximately 89.87%. This metric indicates the model's effectiveness in classifying protest-related images based on the training provided. Visualized the training and validation accuracy over the epochs, observing that the model's performance improved consistently during training, with minimal overfitting observed. Saved the trained model to Google Drive for future use, ensuring the results are reproducible and the model can be further refined if needed. Though not fully implemented in this initial phase, data augmentation techniques such as rotation, flipping, and brightness adjustments were considered to enhance model robustness. These techniques can be integrated in future iterations to potentially improve performance. The current model focuses on binary classification tasks. Future enhancements could involve extending the architecture to handle multi-label classification for more detailed analysis of protest imagery. While the primary focus was on image classification, incorporating sentiment analysis of accompanying textual data using Natural Language Processing (NLP) could provide a more comprehensive understanding of the protest events. Implement data augmentation techniques to improve the model's robustness and generalization capabilities. Explore different architectures, optimization algorithms, and hyperparameters to further enhance model performance. Extend the model to handle multi-label classification tasks, capturing more detailed attributes from the protest images. Incorporate sentiment analysis of textual data to provide a richer context to the visual analysis, enhancing the overall insights into protest activities and sentiments. The initial results demonstrate the potential of a multi-task CNN in effectively analyzing social media protest imagery. The model's ability to classify protest-related images with high accuracy lays a strong foundation for further enhancements and more comprehensive analysis, contributing valuable insights into the dynamics of protest events as captured on social media platforms.

## Remaining Work and Challenges

The remaining work for this project involves several key tasks and addresses potential challenges to enhance the model's performance and comprehensiveness. First, implementing data augmentation techniques such as rotation, flipping, and brightness adjustments is crucial to improve the model's robustness and generalization capabilities. This step aims to create a more diverse training dataset, helping the model learn invariant features and perform better on unseen data. Second, hyperparameter tuning and exploring different CNN architectures or optimization algorithms can further refine the model's accuracy and efficiency. This process involves experimenting with various learning rates, batch sizes, and layer configurations to find the optimal settings for our specific classification tasks. Third, extending the model to handle multi-label classification will allow us to capture more detailed attributes from the protest images, such as specific actions, crowd sizes, and detailed emotional expressions. Integrating Natural Language Processing (NLP) techniques to analyze accompanying textual data from social media posts will also be essential. This integration will provide a richer context, linking visual data with textual sentiments and enhancing the overall analysis. 

The primary challenges include managing the computational resources required for extensive data augmentation and model tuning. Training deep learning models, especially with augmented data and complex architectures, demands significant processing power and memory, which can be a limiting factor. Ensuring that the model does not overfit during the extended training process is another challenge, necessitating careful monitoring and validation. Additionally, combining image and text data poses its own set of challenges, including aligning and synchronizing multimodal inputs and effectively merging the insights derived from each modality. Finally, addressing any potential biases in the dataset to ensure fair and accurate predictions across different protest contexts and demographics is critical. Overcoming these challenges will be pivotal in delivering a robust, accurate, and insightful model capable of comprehensive protest imagery analysis.

## References 
- https://dl.acm.org/doi/pdf/10.1145/3123266.3123282
- https://github.com/wondonghyeon/protest-detection-violence-estimation

Donghyeon Won, Zachary C. Steinert-Threlkeld, and Jungseock Joo. 2017. Protest Activity Detection and Perceived Violence Estimation from Social Media Images. In Proceedings of the 25th ACM International Conference on Multimedia. ACM, 786-794.

